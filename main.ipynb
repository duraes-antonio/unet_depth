{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# U-Net Depth (Projeto de dissertação)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clone o repositório da dissertação\n",
    "!git clone \"https://github.com/duraes-antonio/unet_depth\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Navegue para o repositório\n",
    "!cd \"unet_depth\"\n",
    "os.chdir(\"unet_depth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baixar dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone \"https://gitlab.com/siddinc/new_depth.git\" \"./data\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Particionar (em treino e teste) e instanciar geradores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from domain.models.data.data_generator import NyuV2Generator\n",
    "from infra.util.dataset import load_nyu_train_paths\n",
    "\n",
    "train_path = \"./data/nyu2_train.csv\"\n",
    "test_path = \"./data/nyu2_test.csv\"\n",
    "batch_size = 4\n",
    "init_lr = 0.0001\n",
    "\n",
    "# Defina a semente usada em operações pseudo-aleatórias (como embaralhamento do dataset)\n",
    "seed = 42\n",
    "\n",
    "partition, y_path_by_x_path = load_nyu_train_paths(train_path, 0.3, seed)\n",
    "\n",
    "training_generator = NyuV2Generator(\n",
    "    path_list=partition['train'], labels=y_path_by_x_path, batch_size=batch_size,\n",
    "    seed=seed\n",
    ")\n",
    "validation_generator = NyuV2Generator(\n",
    "    path_list=partition['validation'], labels=y_path_by_x_path, batch_size=batch_size,\n",
    "    seed=seed\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instanciar modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instanciar serviços para persistência de resultados e blob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from infra.services.model_storage_service_google_drive import ModelStorageServiceGoogleDrive\n",
    "from domain.services.model_storage_service import ModelStorageService\n",
    "from infra.services.blob_storage.google_drive_blob_storage_service import GoogleDriveBlobStorageService\n",
    "from infra.services.test_case_execution_service_mongodb import TestCaseExecutionServiceMongoDB\n",
    "from infra.services.test_case_service_mongodb import TestCaseServiceMongoDB\n",
    "from domain.services.test_case_execution_service import TestCaseExecutionService\n",
    "from domain.services.blob_storage_service import BlobStorageService\n",
    "from domain.services.test_case_service import TestCaseService\n",
    "\n",
    "DB_NAME = 'unet_depth'\n",
    "\n",
    "test_case_serv: TestCaseService = TestCaseServiceMongoDB(DB_NAME)\n",
    "execution_serv: TestCaseExecutionService = TestCaseExecutionServiceMongoDB(DB_NAME)\n",
    "\n",
    "blob_service: BlobStorageService = GoogleDriveBlobStorageService()\n",
    "model_storage: ModelStorageService = ModelStorageServiceGoogleDrive(blob_service)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Buscar caso de teste para executar e modelo salvo previamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from domain.models.test_case import TestCaseState\n",
    "\n",
    "last_test_case = test_case_serv.get_first_available()\n",
    "\n",
    "# Se não tiver nenhum caso disponível, finalize a execução\n",
    "if last_test_case is None:\n",
    "    print('Todos casos de teste finalizados!')\n",
    "    exit(0)\n",
    "    raise InterruptedError()\n",
    "\n",
    "test_case_serv.update_state(last_test_case['id'], TestCaseState.Busy)\n",
    "\n",
    "# Buscar última execução do caso de teste\n",
    "last_execution = execution_serv.get_last_execution(last_test_case['id'])\n",
    "\n",
    "# Buscar último blob do modelo atualizado\n",
    "if last_execution:\n",
    "    model_id = last_execution['model_id']\n",
    "\n",
    "    if model_id:\n",
    "        model_storage.recover(model_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'print_test_case' from 'infra.util.output' (C:\\Users\\AEVO\\Desktop\\pessoal\\IFES\\dissertacao\\unet_depth\\infra\\util\\output.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minfra\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moutput\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_test_case\n\u001B[0;32m      3\u001B[0m print_test_case(last_test_case)\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'print_test_case' from 'infra.util.output' (C:\\Users\\AEVO\\Desktop\\pessoal\\IFES\\dissertacao\\unet_depth\\infra\\util\\output.py)"
     ]
    }
   ],
   "source": [
    "from infra.util.output import print_test_case\n",
    "\n",
    "print_test_case(last_test_case)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from typing import Optional, Tuple\n",
    "from domain.models.network import Networks\n",
    "from domain.models.test_case import TestCase\n",
    "from keras_unet_collection import models\n",
    "\n",
    "\n",
    "# Instanciar modelo com base no caso de teste\n",
    "def get_model(test_case: TestCase, input_shape: Tuple[int, int, int]) -> Optional[Model]:\n",
    "    backbone = (test_case['backbone']).value\n",
    "    use_imagenet_weights = test_case['use_imagenet_weights']\n",
    "    network = test_case['network']\n",
    "\n",
    "    n_labels = 1\n",
    "    filter_num = [32, 64, 128, 256, 512]\n",
    "    activation = 'ReLU'\n",
    "    out_activation = 'Sigmoid'\n",
    "    weights = 'imagenet' if use_imagenet_weights else None\n",
    "    pool = False\n",
    "    unpool = True\n",
    "    batch_norm = True\n",
    "\n",
    "    if network == Networks.UNet:\n",
    "        return models.unet_2d(\n",
    "            input_shape, filter_num=filter_num, n_labels=n_labels, stack_num_down=2,\n",
    "            stack_num_up=2, activation=activation, output_activation=out_activation,\n",
    "            batch_norm=batch_norm, pool=pool, unpool=unpool, backbone=backbone,\n",
    "            weights=weights, freeze_backbone=True, freeze_batch_norm=True,\n",
    "        )\n",
    "\n",
    "    if network == Networks.AttentionUNet:\n",
    "        return models.att_unet_2d(\n",
    "            input_shape, filter_num=filter_num, n_labels=n_labels, stack_num_down=2,\n",
    "            stack_num_up=2, activation=activation, atten_activation='ReLU', attention='add',\n",
    "            output_activation=out_activation, batch_norm=batch_norm, pool=pool,\n",
    "            unpool=unpool, backbone=backbone, weights=weights, freeze_backbone=True,\n",
    "            freeze_batch_norm=True, name='attunet'\n",
    "        )\n",
    "    #\n",
    "    # if network == Networks.SwinUNet:\n",
    "    #     return models.swin_unet_2d(\n",
    "    #         input_size=input_shape, filter_num_begin, n_labels=n_labels, depth=4,\n",
    "    #         stack_num_down=2, stack_num_up=2, patch_size, num_heads,\n",
    "    #         window_size, num_mlp, output_activation=out_activation, shift_window=True,\n",
    "    #         name='swin_unet'\n",
    "    #     )\n",
    "\n",
    "    if network == Networks.TransUNet:\n",
    "        return models.transunet_2d(\n",
    "            input_size=input_shape, filter_num=filter_num, n_labels=n_labels,\n",
    "            stack_num_down=2, stack_num_up=2, embed_dim=768, num_mlp=3072,\n",
    "            num_heads=12, num_transformer=12, activation=activation, mlp_activation='GELU',\n",
    "            output_activation=out_activation, batch_norm=batch_norm, pool=pool,\n",
    "            unpool=unpool, backbone=backbone, weights=weights, freeze_backbone=True,\n",
    "            freeze_batch_norm=True, name='transunet'\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Invalid network '{network}'\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparar modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definir métricas e função de perda customizadas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "def poly_decay(epoch):\n",
    "    max_epochs = epochs\n",
    "    base_lr = init_lr\n",
    "    power = 1.0\n",
    "    return base_lr * (1 - (epoch / float(max_epochs))) ** power\n",
    "\n",
    "\n",
    "def depth_loss(y_true, y_pred):\n",
    "    w1, w2, w3 = 1.0, 1.0, 0.1\n",
    "\n",
    "    l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\n",
    "\n",
    "    dy_true, dx_true = tf.image.image_gradients(y_true)\n",
    "    dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n",
    "    l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)\n",
    "\n",
    "    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, 1.0)) * 0.5, 0, 1)\n",
    "\n",
    "    return (w1 * l_ssim) + (w2 * K.mean(l_edges)) + (w3 * K.mean(l_depth))\n",
    "\n",
    "\n",
    "def depth_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definir callbacks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = last_test_case['network'].value\n",
    "optimizer: str = last_test_case['optimizer'].value\n",
    "backbone = str(last_test_case['backbone'].value).lower()\n",
    "use_image_net = int(last_test_case['use_imagenet_weights'])\n",
    "epoch = last_execution['last_epoch'] + 1 if last_execution else 1\n",
    "\n",
    "# Exemplo de formato: 'attention-unet_epoch-15_adam_resnet-101_imagenet-0'\n",
    "model_name = f'{network}_epoch-{epoch}_{optimizer}_{backbone}_imagenet-{use_image_net}'\n",
    "csv_log_path = f'{model_name}.log'\n",
    "\n",
    "project_directory = 'unet_depth'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.keras.custom_callbacks import CSVResultsSave, TrainedModelSave\n",
    "from datetime import datetime\n",
    "from tensorflow.python.keras.callbacks import LearningRateScheduler, EarlyStopping, TensorBoard, CSVLogger\n",
    "\n",
    "tensorboard_log_path = \"logs/fit/\"\n",
    "tensorboard_current_log_path = tensorboard_log_path + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "callbacks = [\n",
    "    LearningRateScheduler(poly_decay),\n",
    "    TensorBoard(log_dir=tensorboard_current_log_path, histogram_freq=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True),\n",
    "    CSVLogger(csv_log_path),\n",
    "    CSVResultsSave(blob_service, csv_log_path),\n",
    "    TrainedModelSave(model_storage, model_name)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compilar e treinar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (640, 480, 3)\n",
    "width, height, = input_shape\n",
    "unet_model = get_model(last_test_case, input_shape)\n",
    "epochs = 50\n",
    "\n",
    "unet_model.compile(loss=depth_loss, metrics=[depth_acc], optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_model.fit(training_generator, validation_data=validation_generator, callbacks=callbacks, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% tensorboard --logdir $tensorboard_log_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avaliar na base de teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.util.dataset import read_nyu_csv\n",
    "\n",
    "img_path_pairs = read_nyu_csv(test_path)\n",
    "labels = {x_path: y_path for x_path, y_path in img_path_pairs}\n",
    "test_paths = [x_path for x_path, y_path in img_path_pairs]\n",
    "partition = {'test': test_paths}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.util.preprocessing import preprocess_image\n",
    "from infra.util.preprocessing import preprocess_depth_map\n",
    "import numpy\n",
    "\n",
    "x_test = numpy.empty((len(test_paths), height, width, 3))\n",
    "y_test = numpy.empty((len(test_paths), height, width, 1))\n",
    "\n",
    "for index, ID in enumerate(partition['test'][:]):\n",
    "    x_test[index,] = preprocess_image(ID)\n",
    "    y_test[index,] = preprocess_depth_map(labels[ID])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_model.evaluate(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot de imagens e predições de exemplo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "\n",
    "predicted = unet_model.predict(x_test)\n",
    "\n",
    "for index in range(len(test_paths) - 600):\n",
    "    # Predição\n",
    "    prediction = predicted[index]\n",
    "    prediction = numpy.squeeze(prediction, axis=-1)\n",
    "    plot.subplot(1, 3, 1)\n",
    "    plot.axis(\"off\")\n",
    "    plot.imshow(prediction, cmap=plot.get_cmap('inferno_r'))\n",
    "\n",
    "    # Ground truth\n",
    "    path = partition['test'][index]\n",
    "    label_path = labels[path]\n",
    "    plot.subplot(1, 3, 2)\n",
    "    plot.axis(\"off\")\n",
    "    target_depth_map = preprocess_depth_map(label_path)\n",
    "    target_depth_map = numpy.squeeze(target_depth_map, axis=-1)\n",
    "    plot.imshow(target_depth_map, cmap=plot.get_cmap('inferno_r'))\n",
    "\n",
    "    # Imagem original\n",
    "    plot.subplot(1, 3, 3)\n",
    "    plot.axis(\"off\")\n",
    "    original_image = preprocess_image(path)\n",
    "    plot.imshow(original_image)\n",
    "\n",
    "    plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# U-Net Depth (Projeto de dissertação)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone o repositório da dissertação\n",
    "!git clone \"https://github.com/duraes-antonio/unet_depth\"\n",
    "\n",
    "# Navegue para o repositório\n",
    "!cd \"unet_depth\"\n",
    "os.chdir(\"unet_depth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "running_remote = True\n",
    "running_on_kaggle = True\n",
    "db_name = 'unet_depth'\n",
    "\n",
    "if running_remote:\n",
    "    !pip install pymongo[srv] dnspython keras-unet-collection python-dotenv imutils\n",
    "    !pip install py-cpuinfo gpuinfo\n",
    "\n",
    "if running_on_kaggle:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    database_url = user_secrets.get_secret(\"DATABASE_URL\")\n",
    "    google_cred_json = ast.literal_eval(user_secrets.get_secret(\"GOOGLE_CREDENTIALS_JSON\"))\n",
    "    google_token_json = ast.literal_eval(user_secrets.get_secret(\"GOOGLE_TOKEN_JSON\"))\n",
    "\n",
    "    with open('google_credentials.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(google_cred_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open('token.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(google_token_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    os.environ[\"DATABASE_URL\"] = database_url\n",
    "    os.environ[\"DB_NAME\"] = db_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instanciar serviços para persistência de resultados e blob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.services.model_storage_service_google_drive import ModelStorageServiceGoogleDrive\n",
    "from domain.services.model_storage_service import ModelStorageService\n",
    "from infra.services.blob_storage.google_drive_blob_storage_service import GoogleDriveBlobStorageService\n",
    "from infra.services.test_case_execution_service_mongodb import TestCaseExecutionServiceMongoDB\n",
    "from infra.services.test_case_service_mongodb import TestCaseServiceMongoDB\n",
    "from domain.services.test_case_execution_service import TestCaseExecutionService\n",
    "from domain.services.blob_storage_service import BlobStorageService\n",
    "from domain.services.test_case_service import TestCaseService\n",
    "\n",
    "test_case_serv: TestCaseService = TestCaseServiceMongoDB(db_name)\n",
    "execution_serv: TestCaseExecutionService = TestCaseExecutionServiceMongoDB(db_name)\n",
    "\n",
    "blob_service: BlobStorageService = GoogleDriveBlobStorageService()\n",
    "model_storage: ModelStorageService = ModelStorageServiceGoogleDrive(blob_service)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Buscar caso de teste para executar e modelo salvo previamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from domain.models.test_case import TestCaseState\n",
    "\n",
    "last_test_case = test_case_serv.get_first_available()\n",
    "\n",
    "# Se não tiver nenhum caso disponível, finalize a execução\n",
    "if last_test_case is None:\n",
    "    print('Todos casos de teste finalizados!')\n",
    "    exit(0)\n",
    "    raise InterruptedError()\n",
    "\n",
    "test_case_serv.update_state(last_test_case['id'], TestCaseState.Busy)\n",
    "\n",
    "# Buscar última execução do caso de teste\n",
    "last_execution = execution_serv.get_last_execution(last_test_case['id'])\n",
    "\n",
    "model_id = None\n",
    "\n",
    "# Buscar último blob do modelo atualizado\n",
    "if last_execution:\n",
    "    model_id = last_execution['model_id']\n",
    "\n",
    "    if model_id:\n",
    "        model_storage.recover(model_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.util.output import print_test_case\n",
    "\n",
    "print_test_case(last_test_case)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baixar dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone \"https://gitlab.com/siddinc/new_depth.git\" \"./data\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Particionar (em treino e teste) e instanciar geradores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from domain.models.data.data_generator import NyuV2Generator\n",
    "from infra.util.dataset import load_nyu_train_paths\n",
    "\n",
    "train_path = \"./data/nyu2_train.csv\"\n",
    "test_path = \"./data/nyu2_test.csv\"\n",
    "batch_size = 4\n",
    "\n",
    "# Defina a semente usada em operações pseudo-aleatórias (como embaralhamento do dataset)\n",
    "seed = 42\n",
    "\n",
    "partition, y_path_by_x_path = load_nyu_train_paths(train_path, 0.3, seed, dataset_usage_percent=0.1)\n",
    "\n",
    "training_generator = NyuV2Generator(partition['train'], y_path_by_x_path, batch_size, seed=seed)\n",
    "validation_generator = NyuV2Generator(partition['validation'], y_path_by_x_path, batch_size, seed=seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instanciar modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from typing import Optional, Tuple\n",
    "from domain.models.network import Networks\n",
    "from domain.models.test_case import TestCase\n",
    "from keras_unet_collection import models\n",
    "\n",
    "\n",
    "# Instanciar modelo com base no caso de teste\n",
    "def get_model(test_case: TestCase, _input_shape: Tuple[int, int, int]) -> Optional[keras.Model]:\n",
    "    _backbone = (test_case['backbone']).value\n",
    "    _network = test_case['network']\n",
    "    use_imagenet_weights = test_case['use_imagenet_weights']\n",
    "\n",
    "    n_labels = 1\n",
    "    filter_num = [64, 128, 256, 512, 1024]\n",
    "    activation = 'ReLU'\n",
    "    out_activation = 'Sigmoid'\n",
    "    weights = 'imagenet' if use_imagenet_weights else None\n",
    "    pool = False\n",
    "    unpool = True\n",
    "    batch_norm = True\n",
    "\n",
    "    if _network == Networks.UNet:\n",
    "        return models.unet_2d(\n",
    "            _input_shape, filter_num=filter_num, n_labels=n_labels, stack_num_down=2,\n",
    "            stack_num_up=2, activation=activation, output_activation=out_activation,\n",
    "            batch_norm=batch_norm, pool=pool, unpool=unpool, backbone=_backbone,\n",
    "            weights=weights, freeze_backbone=True, freeze_batch_norm=True,\n",
    "        )\n",
    "\n",
    "    if _network == Networks.AttentionUNet:\n",
    "        return models.att_unet_2d(\n",
    "            _input_shape, filter_num=filter_num, n_labels=n_labels, stack_num_down=2,\n",
    "            stack_num_up=2, activation=activation, atten_activation='ReLU', attention='add',\n",
    "            output_activation=out_activation, batch_norm=batch_norm, pool=pool,\n",
    "            unpool=unpool, backbone=_backbone, weights=weights, freeze_backbone=True,\n",
    "            freeze_batch_norm=True, name='attunet'\n",
    "        )\n",
    "    #\n",
    "    # if network == Networks.SwinUNet:\n",
    "    #     return models.swin_unet_2d(\n",
    "    #         input_size=_input_shape, filter_num_begin, n_labels=n_labels, depth=4,\n",
    "    #         stack_num_down=2, stack_num_up=2, patch_size, num_heads,\n",
    "    #         window_size, num_mlp, output_activation=out_activation, shift_window=True,\n",
    "    #         name='swin_unet'\n",
    "    #     )\n",
    "\n",
    "    if _network == Networks.TransUNet:\n",
    "        return models.transunet_2d(\n",
    "            input_size=_input_shape, filter_num=filter_num, n_labels=n_labels,\n",
    "            stack_num_down=2, stack_num_up=2, embed_dim=768, num_mlp=3072,\n",
    "            num_heads=12, num_transformer=12, activation=activation, mlp_activation='GELU',\n",
    "            output_activation=out_activation, batch_norm=batch_norm, pool=pool,\n",
    "            unpool=unpool, backbone=_backbone, weights=weights, freeze_backbone=True,\n",
    "            freeze_batch_norm=True, name='transunet'\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Invalid network '{_network}'\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparar modelo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definir configuração"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "input_shape = (image_size, image_size, 3)\n",
    "width, height, _ = input_shape\n",
    "epochs = 50\n",
    "initial_lr = 0.0001"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definir callbacks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = last_test_case['network'].value\n",
    "optimizer: str = last_test_case['optimizer'].value\n",
    "backbone = str(last_test_case['backbone'].value).lower()\n",
    "use_image_net = int(last_test_case['use_imagenet_weights'])\n",
    "epoch = last_execution['last_epoch'] + 1 if last_execution else 1\n",
    "\n",
    "# Exemplo de formato: 'attention-unet_epoch-15_adam_resnet-101_imagenet-0'\n",
    "trained_model_name = f'{network}_{optimizer}_{backbone}_imagenet-{use_image_net}'\n",
    "csv_log_name = f'{network}_epoch-{epoch}_{optimizer}_{backbone}_imagenet-{use_image_net}.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.keras.metrics import build_poly_decay\n",
    "from infra.keras.callbacks import CSVResultsSave, TrainedModelSaveRemote\n",
    "from datetime import datetime\n",
    "from tensorflow.python.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, CSVLogger, ModelCheckpoint\n",
    "\n",
    "tensorboard_log_path = \"logs/fit/\"\n",
    "tensorboard_current_log_path = tensorboard_log_path + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=trained_model_name,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "save_model_remote_callback = TrainedModelSaveRemote(\n",
    "    model_storage,\n",
    "    execution_serv,\n",
    "    test_case_serv,\n",
    "    trained_model_name,\n",
    "    last_test_case['id'],\n",
    "    epoch\n",
    ")\n",
    "\n",
    "poly_decay = build_poly_decay(epochs, initial_lr)\n",
    "callbacks = [\n",
    "    LearningRateScheduler(poly_decay),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True),\n",
    "    CSVLogger(csv_log_name),\n",
    "    CSVResultsSave(blob_service, csv_log_name, epoch),\n",
    "    model_checkpoint_callback,\n",
    "    save_model_remote_callback,\n",
    "    TensorBoard(log_dir=tensorboard_current_log_path, histogram_freq=1),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compilar e treinar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.keras.loss import depth_loss\n",
    "from infra.keras.metrics import depth_acc\n",
    "from tensorflow import keras\n",
    "\n",
    "if model_id is not None and last_execution is not None:\n",
    "    custom_objects = {'poly_decay': poly_decay, 'depth_loss': depth_loss, 'depth_acc': depth_acc}\n",
    "    unet_model = keras.models.load_model(last_execution['model_name'], custom_objects)\n",
    "\n",
    "else:\n",
    "    unet_model = get_model(last_test_case, input_shape)\n",
    "\n",
    "unet_model.compile(loss=depth_loss, metrics=[depth_acc], optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_model.fit(training_generator, validation_data=validation_generator, callbacks=callbacks, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tensorboard_log_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avaliar na base de teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.util.dataset import read_nyu_csv\n",
    "\n",
    "img_path_pairs = read_nyu_csv(test_path)\n",
    "labels = {x_path: y_path for x_path, y_path in img_path_pairs}\n",
    "test_paths = [x_path for x_path, y_path in img_path_pairs]\n",
    "partition = {'test': test_paths}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from infra.util.preprocessing import preprocess_image\n",
    "from infra.util.preprocessing import preprocess_depth_map\n",
    "import numpy\n",
    "\n",
    "x_test = numpy.empty((len(test_paths), height, width, 3))\n",
    "y_test = numpy.empty((len(test_paths), height, width, 1))\n",
    "\n",
    "for index, ID in enumerate(partition['test'][:]):\n",
    "    x_test[index,] = preprocess_image(ID)\n",
    "    y_test[index,] = preprocess_depth_map(labels[ID])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_model.evaluate(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot de imagens e predições de exemplo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "\n",
    "predicted = unet_model.predict(x_test)\n",
    "\n",
    "for index in range(len(test_paths) - 600):\n",
    "    # Predição\n",
    "    prediction = predicted[index]\n",
    "    prediction = numpy.squeeze(prediction, axis=-1)\n",
    "    plot.subplot(1, 3, 1)\n",
    "    plot.axis(\"off\")\n",
    "    plot.imshow(prediction, cmap=plot.get_cmap('viridis_r'))\n",
    "\n",
    "    # Ground truth\n",
    "    path = partition['test'][index]\n",
    "    label_path = labels[path]\n",
    "    plot.subplot(1, 3, 2)\n",
    "    plot.axis(\"off\")\n",
    "    target_depth_map = preprocess_depth_map(label_path)\n",
    "    target_depth_map = numpy.squeeze(target_depth_map, axis=-1)\n",
    "    plot.imshow(target_depth_map, cmap=plot.get_cmap('inferno_r'))\n",
    "\n",
    "    # Imagem original\n",
    "    plot.subplot(1, 3, 3)\n",
    "    plot.axis(\"off\")\n",
    "    original_image = preprocess_image(path)\n",
    "    plot.imshow(original_image)\n",
    "\n",
    "    plot.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
